# -*- coding: utf-8 -*-
"""Copy of Detic_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j_Zv7RbbWHw3q8lmHtPQNZEdYEUyPYhU

# Detecting Twenty-thousand Classes using Image-level Supervision

<img align="center" src="https://github.com/facebookresearch/Detic/raw/main/docs/teaser.jpeg" width="800">

This is a colab demo of using Detic (A **Det**ector with **i**mage **c**lasses). We will use the pretrained Detic models to run object detection on both the detector's vocabulary and any user-specifid vocabulary.

This demo is modified from the [detectron2 colab tutorial](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5).

You can make a copy of this tutorial by "File -> Open in playground mode" and make changes there. __DO NOT__ request access to this tutorial.
"""

# Install detectron2
import torch
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]

# Some basic setup:
# Setup detectron2 logger
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import sys
import numpy as np
import os, json, cv2, random
from cv2 import imshow as cv2_imshow

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

from pathlib import Path
DETIC_DIR = Path.cwd() / "third_party/Detic" # location of Detic directory from root of project
IMG_DIR = Path.cwd() / "cv/imgs" # location of images directory from root of project

# sys.path.insert(0, DETIC_DIR / 'third_party')
# sys.path.insert(0, DETIC_DIR / 'third_party/Deformable-DETR')

os.chdir(DETIC_DIR) # switch to Detic directory for convenience of referencing later paths

# Detic libraries
from centernet.config import add_centernet_config
from detic.config import add_detic_config
from detic.modeling.utils import reset_cls_test
from detic.modeling.text.text_encoder import build_text_encoder

# Build the detector and download our pretrained weights
cfg = get_cfg()
add_centernet_config(cfg)
add_detic_config(cfg)
cfg.merge_from_file("configs/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.yaml")
cfg.MODEL.WEIGHTS = 'https://dl.fbaipublicfiles.com/detic/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth'
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg.MODEL.ROI_BOX_HEAD.ZEROSHOT_WEIGHT_PATH = 'rand'
cfg.MODEL.ROI_HEADS.ONE_CLASS_PER_PROPOSAL = True # For better visualization purpose. Set to False for all classes.
# cfg.MODEL.DEVICE='cpu' # uncomment this to use cpu-only mode.
predictor = DefaultPredictor(cfg)

# Setup the model's vocabulary using build-in datasets

BUILDIN_CLASSIFIER = {
    'lvis': 'datasets/metadata/lvis_v1_clip_a+cname.npy',
    'objects365': 'datasets/metadata/o365_clip_a+cnamefix.npy',
    'openimages': 'datasets/metadata/oid_clip_a+cname.npy',
    'coco': 'datasets/metadata/coco_clip_a+cname.npy',
}

BUILDIN_METADATA_PATH = {
    'lvis': 'lvis_v1_val',
    'objects365': 'objects365_v2_val',
    'openimages': 'oid_val_expanded',
    'coco': 'coco_2017_val',
}

# Image name to desired custom vocabulary mapping
IMAGE_CUSTOM_VOCABS = {
    "computer_desk.jpg": [],
    "hospital_cleaning_pov.jpg": ['hand', 'wash rag', 'bucket', 'spray bottle', 'latex gloves', 'tray', 'chair', 'person'],
    "fastfood_clean.jpg": ['cutting board', 'person', 'dispenser', 'cheese', 'burger bun', 'lettuce', 'hand', 'countertop'],
    "bathroom_cleaning.jpg": ['mop', 'mop bucket', 'trash can', 'sink', 'floor', 'bathroom stall'],
    "cooking_vid_hand_path.png": ['bowl', 'chopsticks', 'hand', 'mug', 'sauce bottle', 'plate', 'glass']
}

# Initialize the image from pathlib Path object
def init_image(image_path: Path):
    image_path_str = (IMG_DIR / image_path).resolve().as_posix() # uses the image directory to create absolute image path
    
    cv2.namedWindow("in", cv2.WINDOW_NORMAL)
    im = cv2.imread(image_path_str)
    cv2_imshow('in', im)
    
    cv2.waitKey(0)
    cv2.destroyAllWindows()

    return im

# Performs instance segmentation on the image using the built-in vocabulary
def predict_built_in_vocabulary(image, vocabulary):
    vocabulary = 'lvis' # change to 'lvis', 'objects365', 'openimages', or 'coco'
    metadata = MetadataCatalog.get(BUILDIN_METADATA_PATH[vocabulary])
    classifier = BUILDIN_CLASSIFIER[vocabulary]
    num_classes = len(metadata.thing_classes)
    reset_cls_test(predictor.model, classifier, num_classes)

    # Run model and show results
    outputs = predictor(image)
    v = Visualizer(image[:, :, ::-1], metadata)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))

    cv2.namedWindow("out", cv2.WINDOW_NORMAL)

    cv2_imshow('out', out.get_image()[:, :, ::-1])
    cv2.waitKey(0)
    cv2.destroyAllWindows() 

# Old method of getting word-embeddings using a pre-trained CLIP model. Not used since it takes up too much memory
def get_clip_embeddings_old(vocabulary, prompt='a '):
    text_encoder = build_text_encoder(pretrain=True)
    text_encoder.eval()
    texts = [prompt + x for x in vocabulary]
    emb = text_encoder(texts).detach().permute(1, 0).contiguous().cuda()
    return emb

# New method of getting embeddings. Uses batch processing to reduce memory usage
def get_clip_embeddings(vocabulary, prompt='a ', batch_size=32):
    text_encoder = build_text_encoder(pretrain=True)
    text_encoder.eval()
    embeddings = []
    for i in range(0, len(vocabulary), batch_size):
        batch = vocabulary[i:i+batch_size]
        if len(batch) < batch_size:
            # pad the last batch with dummy values
            batch += [''] * (batch_size - len(batch))
        texts = [prompt + x for x in batch]
        emb = text_encoder(texts).detach().permute(1, 0).contiguous()
        embeddings.append(emb)
    return torch.cat(embeddings).view(512, -1)

# Performs instance segmentation on the image using a custom user-defined vocabulary
def predict_custom_vocabulary(image, classes):
    metadata = MetadataCatalog.get("__unused")
    metadata.thing_classes = classes

    classifier = get_clip_embeddings(metadata.thing_classes)
    print("got embeddings")
    print(classifier.shape)
    num_classes = len(metadata.thing_classes)
    reset_cls_test(predictor.model, classifier, num_classes)

    # Reset visualization threshold
    output_score_threshold = 0.3
    for cascade_stages in range(len(predictor.model.roi_heads.box_predictor)):
        predictor.model.roi_heads.box_predictor[cascade_stages].test_score_thresh = output_score_threshold

    # Run model and show results
    outputs = predictor(image)

    print("finished predicting outputs")

    v = Visualizer(image[:, :, ::-1], metadata)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    
    cv2.namedWindow("out2", cv2.WINDOW_NORMAL)
    
    cv2_imshow('out2', out.get_image()[:, :, ::-1])
    cv2.waitKey(0)
    cv2.destroyAllWindows() 

    # look at the outputs.
    # See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification
    print(outputs["instances"].pred_classes) # class index
    print([metadata.thing_classes[x] for x in outputs["instances"].pred_classes.cuda().tolist()]) # class names
    print(outputs["instances"].scores)
    print(outputs["instances"].pred_boxes)

if __name__ == '__main__':
    # image_path = "../imgs/hospital_cleaning_pov.jpg"
    image_path = "cooking_vid_hand_path.png"

    im = init_image(image_path)

    predict_built_in_vocabulary(im, 'lvis')
    predict_custom_vocabulary(im, IMAGE_CUSTOM_VOCABS[image_path])