{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "import argparse\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "import cv2\n",
    "import sys\n",
    "import mss\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data.detection_utils import read_image\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "sys.path.insert(0, 'Detic/third_party/CenterNet2/')\n",
    "sys.path.insert(0, 'Detic/')\n",
    "# sys.path.insert(0, 'Detic/')\n",
    "from centernet.config import add_centernet_config\n",
    "from detic.config import add_detic_config\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.engine.defaults import DefaultPredictor\n",
    "from detectron2.utils.video_visualizer import VideoVisualizer\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer, _create_text_labels\n",
    "\n",
    "from detic.modeling.utils import reset_cls_test\n",
    "import detectron2.data.transforms as T\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILDIN_CLASSIFIER = {\n",
    "    'lvis': 'Detic/datasets/metadata/lvis_v1_clip_a+cname.npy',\n",
    "    'objects365': 'Detic/datasets/metadata/o365_clip_a+cnamefix.npy',\n",
    "    'openimages': 'Detic/datasets/metadata/oid_clip_a+cname.npy',\n",
    "    'coco': 'Detic/datasets/metadata/coco_clip_a+cname.npy',\n",
    "}\n",
    "\n",
    "BUILDIN_METADATA_PATH = {\n",
    "    'lvis': 'lvis_v1_val',\n",
    "    'objects365': 'objects365_v2_val',\n",
    "    'openimages': 'oid_val_expanded',\n",
    "    'coco': 'coco_2017_val',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cfg(config_path, weights_path):\n",
    "    cfg = get_cfg()\n",
    "    cfg.MODEL.DEVICE=\"cpu\"\n",
    "    add_centernet_config(cfg)\n",
    "    add_detic_config(cfg)\n",
    "    cfg.merge_from_file(config_path)\n",
    "    cfg.merge_from_list([\"MODEL.WEIGHTS\",weights_path])\n",
    "    # Set score_threshold for builtin models\n",
    "    cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.6\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n",
    "    cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = 0.6\n",
    "    cfg.MODEL.ROI_BOX_HEAD.ZEROSHOT_WEIGHT_PATH = 'rand' # load later\n",
    "    # if not args.pred_all_class:\n",
    "    cfg.MODEL.ROI_HEADS.ONE_CLASS_PER_PROPOSAL = True\n",
    "    cfg.MODEL.ROI_BOX_HEAD.CAT_FREQ_PATH = 'Detic/'+ cfg.MODEL.ROI_BOX_HEAD.CAT_FREQ_PATH\n",
    "    cfg.freeze()\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = setup_cfg(\"/Users/ajaybati/Documents/mimic/mimic/grasping/Detic/configs/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.yaml\",\n",
    "          \"/Users/ajaybati/Documents/mimic/mimic/grasping/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetadataCatalog.get(\n",
    "            BUILDIN_METADATA_PATH[\"lvis\"])\n",
    "classifier = BUILDIN_CLASSIFIER[\"lvis\"]\n",
    "num_classes = len(metadata.thing_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaybati/miniconda3/envs/mimic/lib/python3.12/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting zs_weight Detic/datasets/metadata/lvis_v1_clip_a+cname.npy\n"
     ]
    }
   ],
   "source": [
    "predictor = DefaultPredictor(cfg)\n",
    "reset_cls_test(predictor.model, classifier, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = cv2.imread(\"dog_bike_car.jpg\") #dog_bike_car.jpg, kitchen.jpeg\n",
    "aug = T.ResizeShortestEdge(\n",
    "    [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(original_image):\n",
    "    height, width = original_image.shape[:2]\n",
    "    image = aug.get_transform(original_image).apply_image(original_image)\n",
    "    image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "    image.to(\"cpu\")\n",
    "    return {\"image\": image, \"height\": height, \"width\": width}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detic_forward(batched_inputs):\n",
    "    with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "        # Apply pre-processing to image.\n",
    "        # height, width = original_image.shape[:2]\n",
    "        # image = aug.get_transform(original_image).apply_image(original_image)\n",
    "        # image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "        # image.to(cfg.MODEL.DEVICE)\n",
    "\n",
    "        # inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "        # batched_inputs = [inputs, inputs]\n",
    "        predictions = predictor.model(batched_inputs)\n",
    "        # model = predictor.model\n",
    "        # images = model.preprocess_image(batched_inputs)\n",
    "        # features = model.backbone(images.tensor)\n",
    "        # proposals, _ = model.proposal_generator(images, features, None)\n",
    "        batched_feats = []\n",
    "        for pred in predictions:\n",
    "            x = pred['instances'].feats\n",
    "            if x.dim() > 2:\n",
    "                x = torch.flatten(x, start_dim=1)\n",
    "            official_feats = predictor.model.roi_heads.box_predictor[-1].cls_score.linear(x)\n",
    "            batched_feats.append(official_feats)\n",
    "        return predictions, batched_feats\n",
    "    # predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to read video and label it with model output predictions\n",
    "cap = cv2.VideoCapture('query2.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "out = cv2.VideoWriter(\n",
    "    os.path.join(f\"query2Out2.mp4\"),\n",
    "    cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "    30, (frame_width, frame_height)\n",
    ")\n",
    "\n",
    "for f in tqdm(range(frame_count)):\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    batched_inputs = [preprocess_img(frame)]\n",
    "    preds, feats = detic_forward(batched_inputs)\n",
    "    predictions = preds[0]\n",
    "    features = feats[0]\n",
    "    labels = _create_text_labels(predictions['instances'].pred_classes.tolist(), predictions['instances'].scores, metadata.get(\"thing_classes\", None))\n",
    "    bboxes = predictions['instances'].pred_boxes.tensor.numpy().astype(int).tolist()\n",
    "    scores = predictions['instances'].scores.tolist()\n",
    "    for i, box in enumerate(bboxes):\n",
    "        if 'knife' in labels[i]:\n",
    "            cv2.rectangle(frame, box[:2], box[2:], (0, 255, 0), 4)\n",
    "            cv2.putText(frame, f'{labels[i]}: {scores[i]:.2f}', (box[0], box[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
    "        else:\n",
    "            cv2.rectangle(frame, box[:2], box[2:], (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f'{labels[i]}: {scores[i]:.2f}', (box[0], box[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 3)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    out.write(frame)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(query1, query2):\n",
    "    \"\"\"\n",
    "    cossim compare query1: nxd with query2: mxd\n",
    "\n",
    "    Output: nxm\n",
    "    \"\"\"\n",
    "    norm1 = torch.linalg.norm(query1,dim=-1, keepdim=True)\n",
    "    norm2 = torch.linalg.norm(query2,dim=-1, keepdim=True).T\n",
    "    second = torch.transpose(query2,0,1)[None,:,:]\n",
    "    return (query1[:,None,:] @ second).squeeze()/(norm1 @ norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(path):\n",
    "    \"\"\"\n",
    "    processes image through model\n",
    "\n",
    "    Input: path to image\n",
    "    Output: get 512d features, lables, integer bboxes, and scores\n",
    "    \"\"\"\n",
    "    frame = cv2.imread(path)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    batched_inputs = [preprocess_img(frame)]\n",
    "    preds, feats = detic_forward(batched_inputs)\n",
    "    predictions = preds[0]\n",
    "    features = feats[0]\n",
    "    labels = _create_text_labels(predictions['instances'].pred_classes.tolist(), predictions['instances'].scores, metadata.get(\"thing_classes\", None))\n",
    "    bboxes = predictions['instances'].pred_boxes.tensor.numpy().astype(int).tolist()\n",
    "    scores = predictions['instances'].scores.tolist()\n",
    "    return features, labels, bboxes, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1, labels1, bboxes1, scores1 = get_outputs('query1.png')\n",
    "features2, labels2, bboxes2, scores2 = get_outputs('query2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['person 91%', 'knife 91%', 'tomato 94%', 'tomato 94%',\n",
       "       'lettuce 77%', 'tomato 87%', 'chopping_board 89%', 'jean 76%',\n",
       "       'button 82%', 'lettuce 70%', 'cucumber 65%', 'cucumber 65%',\n",
       "       'tomato 87%', 'button 82%', 'cucumber 65%', 'tomato 87%',\n",
       "       'tomato 87%', 'button 82%', 'tomato 94%', 'chopping_board 89%',\n",
       "       'tomato 87%', 'cucumber 65%', 'cucumber 65%', 'tomato 87%',\n",
       "       'cucumber 65%', 'cucumber 65%', 'cucumber 90%', 'cucumber 65%'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = cos_sim(features1, features2).numpy() #cosine similarity between features1, features2 -> output=#features1x#features2\n",
    "np.take(labels2, np.argmax(sim, axis=-1)) #for each detected item in features1, get closest item in features2, then index through features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['person 94%', 'knife 92%', 'tomato 89%', 'apple 85%', 'bowl 82%',\n",
       "       'bell_pepper 80%', 'chopping_board 79%', 'pot 78%', 'button 76%',\n",
       "       'bowl 76%', 'green_onion 74%', 'bell_pepper 73%', 'grape 73%',\n",
       "       'button 72%', 'green_onion 71%', 'bread 70%', 'carrot 70%',\n",
       "       'button 70%', 'cherry 69%', 'baguet 69%', 'tomato 69%',\n",
       "       'garlic 65%', 'brussels_sprouts 65%', 'chili_(vegetable) 63%',\n",
       "       'garlic 63%', 'garlic 62%', 'green_onion 61%', 'green_onion 60%'],\n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
