{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "import argparse\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "import cv2\n",
    "import sys\n",
    "import mss\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data.detection_utils import read_image\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "sys.path.insert(0, 'Detic/third_party/CenterNet2/')\n",
    "sys.path.insert(0, 'Detic/')\n",
    "# sys.path.insert(0, 'Detic/')\n",
    "from centernet.config import add_centernet_config\n",
    "from detic.config import add_detic_config\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.engine.defaults import DefaultPredictor\n",
    "from detectron2.utils.video_visualizer import VideoVisualizer\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer, _create_text_labels\n",
    "\n",
    "from detic.modeling.utils import reset_cls_test\n",
    "import detectron2.data.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILDIN_CLASSIFIER = {\n",
    "    'lvis': 'Detic/datasets/metadata/lvis_v1_clip_a+cname.npy',\n",
    "    'objects365': 'Detic/datasets/metadata/o365_clip_a+cnamefix.npy',\n",
    "    'openimages': 'Detic/datasets/metadata/oid_clip_a+cname.npy',\n",
    "    'coco': 'Detic/datasets/metadata/coco_clip_a+cname.npy',\n",
    "}\n",
    "\n",
    "BUILDIN_METADATA_PATH = {\n",
    "    'lvis': 'lvis_v1_val',\n",
    "    'objects365': 'objects365_v2_val',\n",
    "    'openimages': 'oid_val_expanded',\n",
    "    'coco': 'coco_2017_val',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cfg(config_path, weights_path):\n",
    "    cfg = get_cfg()\n",
    "    cfg.MODEL.DEVICE=\"cpu\"\n",
    "    add_centernet_config(cfg)\n",
    "    add_detic_config(cfg)\n",
    "    cfg.merge_from_file(config_path)\n",
    "    cfg.merge_from_list([\"MODEL.WEIGHTS\",weights_path])\n",
    "    # Set score_threshold for builtin models\n",
    "    cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.6\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n",
    "    cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = 0.6\n",
    "    cfg.MODEL.ROI_BOX_HEAD.ZEROSHOT_WEIGHT_PATH = 'rand' # load later\n",
    "    # if not args.pred_all_class:\n",
    "    cfg.MODEL.ROI_HEADS.ONE_CLASS_PER_PROPOSAL = True\n",
    "    cfg.MODEL.ROI_BOX_HEAD.CAT_FREQ_PATH = 'Detic/'+ cfg.MODEL.ROI_BOX_HEAD.CAT_FREQ_PATH\n",
    "    cfg.freeze()\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = setup_cfg(\"/Users/ajaybati/Documents/mimic/mimic/grasping/Detic/configs/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.yaml\",\n",
    "#           \"/Users/ajaybati/Documents/mimic/mimic/grasping/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth\")\n",
    "\n",
    "cfg = setup_cfg(\"C:\\\\Users\\\\arshs\\\\OneDrive\\\\Documents\\\\GitHub\\\\mimic\\\\third_party\\\\Detic\\\\configs\\\\Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.yaml\", \"https://dl.fbaipublicfiles.com/detic/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetadataCatalog.get(\n",
    "            BUILDIN_METADATA_PATH[\"lvis\"])\n",
    "classifier = BUILDIN_CLASSIFIER[\"lvis\"]\n",
    "num_classes = len(metadata.thing_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Detic/datasets/metadata/lvis_v1_train_cat_info.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m reset_cls_test(predictor\u001b[38;5;241m.\u001b[39mmodel, classifier, num_classes)\n",
      "File \u001b[1;32mc:\\users\\arshs\\onedrive\\documents\\github\\mimic\\src\\detectron2\\detectron2\\engine\\defaults.py:282\u001b[0m, in \u001b[0;36mDefaultPredictor.__init__\u001b[1;34m(self, cfg)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg):\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mclone()  \u001b[38;5;66;03m# cfg can be modified by model\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mDATASETS\u001b[38;5;241m.\u001b[39mTEST):\n",
      "File \u001b[1;32mc:\\users\\arshs\\onedrive\\documents\\github\\mimic\\src\\detectron2\\detectron2\\modeling\\meta_arch\\build.py:22\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03mBuild the whole model architecture, defined by ``cfg.MODEL.META_ARCHITECTURE``.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03mNote that it does not load any weights from ``cfg``.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m meta_arch \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mMETA_ARCHITECTURE\n\u001b[1;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMETA_ARCH_REGISTRY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_arch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(cfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mDEVICE))\n\u001b[0;32m     24\u001b[0m _log_api_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodeling.meta_arch.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m meta_arch)\n",
      "File \u001b[1;32mc:\\users\\arshs\\onedrive\\documents\\github\\mimic\\src\\detectron2\\detectron2\\config\\config.py:189\u001b[0m, in \u001b[0;36mconfigurable.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass with @configurable must have a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom_config\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _called_with_cfg(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 189\u001b[0m     explicit_args \u001b[38;5;241m=\u001b[39m _get_args_from_config(from_config_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    190\u001b[0m     init_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexplicit_args)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\arshs\\onedrive\\documents\\github\\mimic\\src\\detectron2\\detectron2\\config\\config.py:245\u001b[0m, in \u001b[0;36m_get_args_from_config\u001b[1;34m(from_config_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m supported_arg_names:\n\u001b[0;32m    244\u001b[0m         extra_kwargs[name] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(name)\n\u001b[1;32m--> 245\u001b[0m ret \u001b[38;5;241m=\u001b[39m from_config_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# forward the other arguments to __init__\u001b[39;00m\n\u001b[0;32m    247\u001b[0m ret\u001b[38;5;241m.\u001b[39mupdate(extra_kwargs)\n",
      "File \u001b[1;32mc:\\users\\arshs\\onedrive\\documents\\github\\mimic\\src\\detic\\detic\\modeling\\meta_arch\\custom_rcnn.py:79\u001b[0m, in \u001b[0;36mCustomRCNN.from_config\u001b[1;34m(cls, cfg)\u001b[0m\n\u001b[0;32m     68\u001b[0m ret\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwith_image_labels\u001b[39m\u001b[38;5;124m'\u001b[39m: cfg\u001b[38;5;241m.\u001b[39mWITH_IMAGE_LABELS,\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_loss_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: cfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mDATASET_LOSS_WEIGHT,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcap_batch_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: cfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mCAP_BATCH_RATIO,\n\u001b[0;32m     77\u001b[0m })\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdynamic_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 79\u001b[0m     ret[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfreq_weight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mload_class_freq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mROI_BOX_HEAD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCAT_FREQ_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mROI_BOX_HEAD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFED_LOSS_FREQ_WEIGHT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     ret[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mROI_HEADS\u001b[38;5;241m.\u001b[39mNUM_CLASSES\n\u001b[0;32m     83\u001b[0m     ret[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_sample_cats\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mNUM_SAMPLE_CATS\n",
      "File \u001b[1;32mc:\\users\\arshs\\onedrive\\documents\\github\\mimic\\src\\detic\\detic\\modeling\\utils.py:9\u001b[0m, in \u001b[0;36mload_class_freq\u001b[1;34m(path, freq_weight)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_class_freq\u001b[39m(\n\u001b[0;32m      8\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/metadata/lvis_v1_train_cat_info.json\u001b[39m\u001b[38;5;124m'\u001b[39m, freq_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     cat_info \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m     cat_info \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     11\u001b[0m         [c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(cat_info, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])])\n\u001b[0;32m     12\u001b[0m     freq_weight \u001b[38;5;241m=\u001b[39m cat_info\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m freq_weight\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Detic/datasets/metadata/lvis_v1_train_cat_info.json'"
     ]
    }
   ],
   "source": [
    "predictor = DefaultPredictor(cfg)\n",
    "reset_cls_test(predictor.model, classifier, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = cv2.imread(\"dog_bike_car.jpg\") #dog_bike_car.jpg, kitchen.jpeg\n",
    "aug = T.ResizeShortestEdge(\n",
    "    [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(original_image):\n",
    "    height, width = original_image.shape[:2]\n",
    "    image = aug.get_transform(original_image).apply_image(original_image)\n",
    "    image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "    image.to(\"cpu\")\n",
    "    return {\"image\": image, \"height\": height, \"width\": width}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detic_forward(batched_inputs):\n",
    "    with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "        # Apply pre-processing to image.\n",
    "        # height, width = original_image.shape[:2]\n",
    "        # image = aug.get_transform(original_image).apply_image(original_image)\n",
    "        # image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "        # image.to(cfg.MODEL.DEVICE)\n",
    "\n",
    "        # inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "        # batched_inputs = [inputs, inputs]\n",
    "        predictions = predictor.model(batched_inputs)\n",
    "        # model = predictor.model\n",
    "        # images = model.preprocess_image(batched_inputs)\n",
    "        # features = model.backbone(images.tensor)\n",
    "        # proposals, _ = model.proposal_generator(images, features, None)\n",
    "        batched_feats = []\n",
    "        for pred in predictions:\n",
    "            x = pred['instances'].feats\n",
    "            if x.dim() > 2:\n",
    "                x = torch.flatten(x, start_dim=1)\n",
    "            official_feats = predictor.model.roi_heads.box_predictor[-1].cls_score.linear(x)\n",
    "            batched_feats.append(official_feats)\n",
    "        return predictions, batched_feats\n",
    "    # predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to read video and label it with model output predictions\n",
    "cap = cv2.VideoCapture('query2.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "out = cv2.VideoWriter(\n",
    "    os.path.join(f\"query2Out2.mp4\"),\n",
    "    cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "    30, (frame_width, frame_height)\n",
    ")\n",
    "\n",
    "for f in tqdm(range(frame_count)):\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    batched_inputs = [preprocess_img(frame)]\n",
    "    preds, feats = detic_forward(batched_inputs)\n",
    "    predictions = preds[0]\n",
    "    features = feats[0]\n",
    "    labels = _create_text_labels(predictions['instances'].pred_classes.tolist(), predictions['instances'].scores, metadata.get(\"thing_classes\", None))\n",
    "    bboxes = predictions['instances'].pred_boxes.tensor.numpy().astype(int).tolist()\n",
    "    scores = predictions['instances'].scores.tolist()\n",
    "    for i, box in enumerate(bboxes):\n",
    "        if 'knife' in labels[i]:\n",
    "            cv2.rectangle(frame, box[:2], box[2:], (0, 255, 0), 4)\n",
    "            cv2.putText(frame, f'{labels[i]}: {scores[i]:.2f}', (box[0], box[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
    "        else:\n",
    "            cv2.rectangle(frame, box[:2], box[2:], (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f'{labels[i]}: {scores[i]:.2f}', (box[0], box[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 3)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    out.write(frame)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(query1, query2):\n",
    "    \"\"\"\n",
    "    cossim compare query1: nxd with query2: mxd\n",
    "\n",
    "    Output: nxm\n",
    "    \"\"\"\n",
    "    norm1 = torch.linalg.norm(query1,dim=-1, keepdim=True)\n",
    "    norm2 = torch.linalg.norm(query2,dim=-1, keepdim=True).T\n",
    "    second = torch.transpose(query2,0,1)[None,:,:]\n",
    "    return (query1[:,None,:] @ second).squeeze()/(norm1 @ norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(path):\n",
    "    \"\"\"\n",
    "    processes image through model\n",
    "\n",
    "    Input: path to image\n",
    "    Output: get 512d features, lables, integer bboxes, and scores\n",
    "    \"\"\"\n",
    "    frame = cv2.imread(path)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    batched_inputs = [preprocess_img(frame)]\n",
    "    preds, feats = detic_forward(batched_inputs)\n",
    "    predictions = preds[0]\n",
    "    features = feats[0]\n",
    "    labels = _create_text_labels(predictions['instances'].pred_classes.tolist(), predictions['instances'].scores, metadata.get(\"thing_classes\", None))\n",
    "    bboxes = predictions['instances'].pred_boxes.tensor.numpy().astype(int).tolist()\n",
    "    scores = predictions['instances'].scores.tolist()\n",
    "    return features, labels, bboxes, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1, labels1, bboxes1, scores1 = get_outputs('query1.png')\n",
    "features2, labels2, bboxes2, scores2 = get_outputs('query2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['person 91%', 'knife 91%', 'tomato 94%', 'tomato 94%',\n",
       "       'lettuce 77%', 'tomato 87%', 'chopping_board 89%', 'jean 76%',\n",
       "       'button 82%', 'lettuce 70%', 'cucumber 65%', 'cucumber 65%',\n",
       "       'tomato 87%', 'button 82%', 'cucumber 65%', 'tomato 87%',\n",
       "       'tomato 87%', 'button 82%', 'tomato 94%', 'chopping_board 89%',\n",
       "       'tomato 87%', 'cucumber 65%', 'cucumber 65%', 'tomato 87%',\n",
       "       'cucumber 65%', 'cucumber 65%', 'cucumber 90%', 'cucumber 65%'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = cos_sim(features1, features2).numpy() #cosine similarity between features1, features2 -> output=#features1x#features2\n",
    "np.take(labels2, np.argmax(sim, axis=-1)) #for each detected item in features1, get closest item in features2, then index through features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['person 94%', 'knife 92%', 'tomato 89%', 'apple 85%', 'bowl 82%',\n",
       "       'bell_pepper 80%', 'chopping_board 79%', 'pot 78%', 'button 76%',\n",
       "       'bowl 76%', 'green_onion 74%', 'bell_pepper 73%', 'grape 73%',\n",
       "       'button 72%', 'green_onion 71%', 'bread 70%', 'carrot 70%',\n",
       "       'button 70%', 'cherry 69%', 'baguet 69%', 'tomato 69%',\n",
       "       'garlic 65%', 'brussels_sprouts 65%', 'chili_(vegetable) 63%',\n",
       "       'garlic 63%', 'garlic 62%', 'green_onion 61%', 'green_onion 60%'],\n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
